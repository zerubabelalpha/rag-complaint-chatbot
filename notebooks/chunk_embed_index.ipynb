{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "cac15547",
            "metadata": {},
            "source": [
                "# RAG Pipeline: Chunking, Embedding, and Indexing\n",
                "\n",
                "This notebook orchestrates the process of converting processed CFPB complaint data into a searchable vector store. \n",
                "\n",
                "**Pipeline Steps:**\n",
                "1. Setup environment and HuggingFace cache.\n",
                "2. Load processed data from disk.\n",
                "3. Convert complaints to LangChain Documents.\n",
                "4. Split documents into smaller semantic chunks.\n",
                "5. Generate embeddings and build a FAISS vector index.\n",
                "6. Verify the index with retrieval tests."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac5844b9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "\n",
                "# 1. Setup PROJECT_ROOT to allow importing from src/\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "# 2. Import config and setup HuggingFace cache\n",
                "from src import config\n",
                "config.setup_hf_cache()\n",
                "\n",
                "# 3. Import required custom modules\n",
                "from src.file_handling import load_processed_data\n",
                "from src.docs import dataframe_to_documents, print_document_sample\n",
                "from src.chunking import chunk_documents, get_chunk_stats\n",
                "from src.vectorstore import create_vector_store, load_vector_store, get_retriever, print_search_results\n",
                "\n",
                "print(\"✓ Imports and setup complete!\")\n",
                "\n",
                "from src.preprocess import create_stratified_sample"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "546f66a0",
            "metadata": {},
            "source": [
                "## 1.1 Stratified Sampling\n",
                "\n",
                "We apply stratified sampling to select a representative subset of 10,000 complaints. This ensures the vector store is built on a manageable dataset while maintaining the proportional distribution of product categories from the filtered dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0f3dbf8f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select a stratified sample of 10k records\n",
                "target_sample_size = 10000\n",
                "print(f\"Applying stratified sampling (Target: {target_sample_size})...\")\n",
                "df = create_stratified_sample(df, target_size=target_sample_size)\n",
                "\n",
                "# Verify final shape\n",
                "print(f\"Sample size for indexing: {len(df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c2c6d447",
            "metadata": {},
            "source": [
                "## 1. Load Processed Data\n",
                "\n",
                "We load the data generated by the EDA notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "222103a6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load cleaned data\n",
                "processed_data_path = config.PROCESSED_DATA_PATH\n",
                "df = load_processed_data(processed_data_path)\n",
                "\n",
                "# Verify document text exists\n",
                "text_col = 'clean_narrative'\n",
                "if text_col in df.columns:\n",
                "    print(f\"✓ Found '{text_col}' for indexing\")\n",
                "    # Show snippet of first valid row\n",
                "    print(f\"Snippet: {df[text_col].iloc[0][:100]}...\")\n",
                "else:\n",
                "    print(f\"❌ ERROR: {text_col} not found in the dataset!\")\n",
                "\n",
                "print(f\"Total records loaded: {len(df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "84b8bfe8",
            "metadata": {},
            "source": [
                "## 2. Convert to LangChain Documents\n",
                "\n",
                "We use `src.docs` to convert rows into structured objects that LangChain understands, preserving metadata for retrieval."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a859c2e4",
            "metadata": {},
            "outputs": [],
            "source": [
                "docs = dataframe_to_documents(df)\n",
                "\n",
                "# Preview a document\n",
                "if docs:\n",
                "    print_document_sample(docs[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "edb60d52",
            "metadata": {},
            "source": [
                "## 3. Chunk the Documents\n",
                "\n",
                "Break long narratives into manageable pieces for better embedding search accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8ab5f67a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Splitting documents into chunks\n",
                "chunks = chunk_documents(docs)\n",
                "\n",
                "# Display chunk stats\n",
                "stats = get_chunk_stats(chunks)\n",
                "print(f\"\\nChunking Stats: {stats}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "83727f03",
            "metadata": {},
            "source": [
                "## 4. Create Embedding and Vector Store (FAISS)\n",
                "\n",
                "This step converts text into high-dimensional vectors and stores them in a local index."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9bc4a65d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and persist vector store\n",
                "# Note: On the first run, this download the model (~80MB)\n",
                "vectorstore = create_vector_store(chunks)\n",
                "print(\"✓ Vector store built and saved to disk.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81820f2f",
            "metadata": {},
            "source": [
                "## 5. Test Loading and Retrieval\n",
                "\n",
                "Verify that we can reload the index from disk and perform a search."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "81557b4f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test loading from disk\n",
                "vectorstore_v2 = load_vector_store()\n",
                "\n",
                "# Test retrieval\n",
                "query = \"unauthorized charge on my credit card\"\n",
                "retriever = get_retriever(vectorstore_v2, k=3)\n",
                "results = retriever.invoke(query)\n",
                "\n",
                "# Display results\n",
                "print_search_results(results, query)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d7808e90",
            "metadata": {},
            "source": [
                "## 6. Explore Vector Store\n",
                "\n",
                "A quick look into the index content."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "acec8d7a",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Total vectors in index: {vectorstore_v2.index.ntotal:,}\")\n",
                "print(f\"Sample chunk metadata from retriever result:\")\n",
                "print(results[0].metadata)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv (3.11.9)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
