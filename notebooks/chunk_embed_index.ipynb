{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# RAG Pipeline: Production-Level Indexing\n",
                "\n",
                "This notebook demonstrates the production-level indexing pipeline for the CSV data and how to leverage pre-built embeddings for large-scale retrieval.\n",
                "\n",
                "### Key features:\n",
                "1. **Alignment with `src/vectorstore.py`**: Using the same optimized logic as the production scripts.\n",
                "2. **Memory-Efficient Processing**: Batch loading for large datasets.\n",
                "3. **Pre-built Integration**: Demonstrating how to load the 1.3M+ records store from `data/complaint_embeddings.parquet`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# allow imports from project root\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "from src import config\n",
                "config.setup_hf_cache()\n",
                "\n",
                "from src import vectorstore\n",
                "from src.chunking import get_chunk_stats\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "print(\"✓ Imports and setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load-logic",
            "metadata": {},
            "source": [
                "## 1. Efficient Vector Store Loading\n",
                "\n",
                "In production, we often avoid re-indexing 1.3M records by loading a pre-persisted FAISS index or building it from raw embeddings in batches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-exec",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Vector store path: {config.VECTOR_STORE_DIR}\")\n",
                "print(f\"Pre-built embeddings: {config.PREBUILT_EMBEDDINGS_PATH}\")\n",
                "\n",
                "# Load existing or build from parquet\n",
                "vs = vectorstore.load_vector_store()\n",
                "print(f\"\\n✓ Vector store active with {vs.index.ntotal:,} vectors.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retrieval-test",
            "metadata": {},
            "source": [
                "## 2. Verification through Semantic Search\n",
                "\n",
                "We test the index with a sample query to ensure retrieval is functional and metadata is correctly preserved."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "search-exec",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"I lost my credit card and there are fraudulent charges\"\n",
                "results = vectorstore.search_similar(vs, query, k=3)\n",
                "\n",
                "vectorstore.print_search_results(results, query)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metadata-deep-dive",
            "metadata": {},
            "source": [
                "### Inspecting Metadata and Chunks\n",
                "\n",
                "Good RAG requires precise metadata tracking (e.g., `chunk_index`, `complaint_id`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "metadata-exec",
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_doc = results[0]\n",
                "print(\"Sample Metadata:\")\n",
                "for k, v in sample_doc.metadata.items():\n",
                "    print(f\"  {k}: {v}\")\n",
                "\n",
                "print(f\"\\nContent Snippet:\\n{sample_doc.page_content[:200]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "batch-build-demo",
            "metadata": {},
            "source": [
                "## 3. Building From Parquet (Internal Logic)\n",
                "\n",
                "To handle 1.3M records without crashing memory, we use `pyarrow` to process the parquet file in batches. This logic is encapsulated in `vectorstore.build_vector_store_from_parquet`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "parquet-logic-show",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example of how batches ARE processed (mental model)\n",
                "# for batch in pf.iter_batches(batch_size=50000):\n",
                "#     df_batch = batch.to_pandas()\n",
                "#     vectorstore.add_embeddings(texts=df_batch['document'], ...)\n",
                "\n",
                "print(\"Batch processing is automatically handled by the src scripts to ensure scalability.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}