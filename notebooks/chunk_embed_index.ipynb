{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# RAG Pipeline: Production-Level Indexing\n",
                "\n",
                "This notebook demonstrates the production-level indexing pipeline for the CSV data and how to leverage pre-built embeddings for large-scale retrieval.\n",
                "\n",
                "### Key features:\n",
                "1. **Alignment with `src/vectorstore.py`**: Using the same optimized logic as the production scripts.\n",
                "2. **Memory-Efficient Processing**: Batch loading for large datasets.\n",
                "3. **Pre-built Integration**: Demonstrating how to load the 1.3M+ records store from `data/complaint_embeddings.parquet`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "setup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[OK] HuggingFace cache set to: c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\models\\hf\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Imports and setup complete!\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# allow imports from project root\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "from src import config\n",
                "config.setup_hf_cache()\n",
                "\n",
                "from src import vectorstore\n",
                "from src.chunking import get_chunk_stats\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "print(\"✓ Imports and setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load-logic",
            "metadata": {},
            "source": [
                "## 1. Efficient Vector Store Loading\n",
                "\n",
                "In production, we often avoid re-indexing 1.3M records by loading a pre-persisted FAISS index or building it from raw embeddings in batches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "load-exec",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vector store path: c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\vector_store\\faiss\n",
                        "Pre-built embeddings: c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\data\\complaint_embeddings.parquet\n",
                        "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
                        "  (First run will download ~80MB to c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\models\\hf)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 854cb151-19ba-40d7-a574-9e82ef81120a)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
                        "Retrying in 1s [Retry 1/5].\n",
                        "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 985820e4-4480-4a59-9eaf-87192eb9ddc5)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
                        "Retrying in 2s [Retry 2/5].\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[OK] Embedding model loaded\n",
                        "Loading existing FAISS index from c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\vector_store\\faiss...\n",
                        "[OK] Vector store loaded (ntotal=1,375,327)\n",
                        "\n",
                        "✓ Vector store active with 1,375,327 vectors.\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Vector store path: {config.VECTOR_STORE_DIR}\")\n",
                "print(f\"Pre-built embeddings: {config.PREBUILT_EMBEDDINGS_PATH}\")\n",
                "\n",
                "# Load existing or build from parquet\n",
                "vs = vectorstore.load_vector_store()\n",
                "print(f\"\\n✓ Vector store active with {vs.index.ntotal:,} vectors.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retrieval-test",
            "metadata": {},
            "source": [
                "## 2. Verification through Semantic Search\n",
                "\n",
                "We test the index with a sample query to ensure retrieval is functional and metadata is correctly preserved."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "search-exec",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "======================================================================\n",
                        "SEARCH RESULTS for: 'I lost my credit card and there are fraudulent charges'\n",
                        "======================================================================\n",
                        "\n",
                        "--- Result 1 ---\n",
                        "Complaint ID: 2695792\n",
                        "Product: Credit card or prepaid card\n",
                        "Category: Credit Card\n",
                        "Issue: Problem with a purchase shown on your statement\n",
                        "Company: SYNCHRONY FINANCIAL\n",
                        "Chunk: 0/1\n",
                        "Content preview:\n",
                        "i lost my credit card and and their are fraudulent charges and transactions of 2500.00 . the charges do n't belong to me. i have never used my credit card....\n",
                        "\n",
                        "--- Result 2 ---\n",
                        "Complaint ID: 8236492\n",
                        "Product: Credit card\n",
                        "Category: Credit Card\n",
                        "Issue: Problem with a purchase shown on your statement\n",
                        "Company: Chime Financial Inc\n",
                        "Chunk: 0/1\n",
                        "Content preview:\n",
                        "i lost my credit debit card and unauthorized charges where submitted to my account....\n",
                        "\n",
                        "--- Result 3 ---\n",
                        "Complaint ID: 1409416\n",
                        "Product: Credit card\n",
                        "Category: Credit Card\n",
                        "Issue: Credit card protection / Debt protection\n",
                        "Company: JPMORGAN CHASE & CO.\n",
                        "Chunk: 0/1\n",
                        "Content preview:\n",
                        "i found fraudulent charges on my credit card....\n",
                        "\n",
                        "======================================================================\n"
                    ]
                }
            ],
            "source": [
                "query = \"I lost my credit card and there are fraudulent charges\"\n",
                "results = vectorstore.search_similar(vs, query, k=3)\n",
                "\n",
                "vectorstore.print_search_results(results, query)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metadata-deep-dive",
            "metadata": {},
            "source": [
                "### Inspecting Metadata and Chunks\n",
                "\n",
                "Good RAG requires precise metadata tracking (e.g., `chunk_index`, `complaint_id`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "metadata-exec",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sample Metadata:\n",
                        "  chunk_index: 0\n",
                        "  company: SYNCHRONY FINANCIAL\n",
                        "  complaint_id: 2695792\n",
                        "  date_received: 2017-10-07\n",
                        "  issue: Problem with a purchase shown on your statement\n",
                        "  product: Credit card or prepaid card\n",
                        "  product_category: Credit Card\n",
                        "  state: AZ\n",
                        "  sub_issue: Card was charged for something you did not purchase with the card\n",
                        "  total_chunks: 1\n",
                        "\n",
                        "Content Snippet:\n",
                        "i lost my credit card and and their are fraudulent charges and transactions of 2500.00 . the charges do n't belong to me. i have never used my credit card....\n"
                    ]
                }
            ],
            "source": [
                "sample_doc = results[0]\n",
                "print(\"Sample Metadata:\")\n",
                "for k, v in sample_doc.metadata.items():\n",
                "    print(f\"  {k}: {v}\")\n",
                "\n",
                "print(f\"\\nContent Snippet:\\n{sample_doc.page_content[:200]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "batch-build-demo",
            "metadata": {},
            "source": [
                "## 3. Building From Parquet (Internal Logic)\n",
                "\n",
                "To handle 1.3M records without crashing memory, we use `pyarrow` to process the parquet file in batches. This logic is encapsulated in `vectorstore.build_vector_store_from_parquet`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "parquet-logic-show",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Batch processing is automatically handled by the src scripts to ensure scalability.\n"
                    ]
                }
            ],
            "source": [
                "# Example of how batches ARE processed (mental model)\n",
                "# for batch in pf.iter_batches(batch_size=50000):\n",
                "#     df_batch = batch.to_pandas()\n",
                "#     vectorstore.add_embeddings(texts=df_batch['document'], ...)\n",
                "\n",
                "print(\"Batch processing is automatically handled by the src scripts to ensure scalability.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv (3.11.9)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
