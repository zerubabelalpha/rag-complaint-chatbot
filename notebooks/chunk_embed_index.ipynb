{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "cac15547",
            "metadata": {},
            "source": [
                "# RAG Pipeline: Chunking, Embedding, and Indexing\n",
                "\n",
                "This notebook orchestrates the process of converting processed CFPB complaint data into a searchable vector store. \n",
                "\n",
                "**Pipeline Steps:**\n",
                "1. Setup environment and HuggingFace cache.\n",
                "2. Load processed data from disk.\n",
                "3. Convert complaints to LangChain Documents.\n",
                "4. Split documents into smaller semantic chunks.\n",
                "5. Generate embeddings and build a FAISS vector index.\n",
                "6. Verify the index with retrieval tests."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "ac5844b9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[OK] HuggingFace cache set to: c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\models\\hf\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Imports and setup complete!\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "\n",
                "# 1. Setup PROJECT_ROOT to allow importing from src/\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "# 2. Import config and setup HuggingFace cache\n",
                "from src import config\n",
                "config.setup_hf_cache()\n",
                "\n",
                "# 3. Import required custom modules\n",
                "from src.file_handling import load_processed_data\n",
                "from src.docs import dataframe_to_documents, print_document_sample\n",
                "from src.chunking import chunk_documents, get_chunk_stats\n",
                "from src.vectorstore import create_vector_store, load_vector_store, get_retriever, print_search_results\n",
                "\n",
                "print(\"✓ Imports and setup complete!\")\n",
                "\n",
                "from src.preprocess import create_stratified_sample"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c2c6d447",
            "metadata": {},
            "source": [
                "## 1. Load Processed Data\n",
                "\n",
                "We load the data generated by the EDA notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "222103a6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[OK] Loaded 12,000 processed DATA from filtered_complaints.csv\n",
                        "✓ Found 'clean_narrative' for indexing\n",
                        "Snippet: during the whole time that i had wells fargo ive experienced on going issues that has never been res...\n",
                        "Total records loaded: 12,000\n"
                    ]
                }
            ],
            "source": [
                "# Load cleaned data\n",
                "processed_data_path = config.PROCESSED_DATA_PATH\n",
                "df = load_processed_data(processed_data_path)\n",
                "\n",
                "# Verify document text exists\n",
                "text_col = 'clean_narrative'\n",
                "if text_col in df.columns:\n",
                "    print(f\"✓ Found '{text_col}' for indexing\")\n",
                "    # Show snippet of first valid row\n",
                "    print(f\"Snippet: {df[text_col].iloc[0][:100]}...\")\n",
                "else:\n",
                "    print(f\"❌ ERROR: {text_col} not found in the dataset!\")\n",
                "\n",
                "print(f\"Total records loaded: {len(df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "546f66a0",
            "metadata": {},
            "source": [
                "## 1.1 Stratified Sampling\n",
                "\n",
                "We apply stratified sampling to select a representative subset of 10,000 complaints. This ensures the vector store is built on a manageable dataset while maintaining the proportional distribution of product categories from the filtered dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "0f3dbf8f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Applying stratified sampling (Target: 10000)...\n",
                        "[OK] Created stratified sample: 10,000 rows\n",
                        "\n",
                        "Proportional Representation Check (Product %):\n",
                        "                                                    Original %  Sample %\n",
                        "Product                                                                 \n",
                        "Checking or savings account                              39.28     39.27\n",
                        "Credit card                                              22.58     22.57\n",
                        "Credit card or prepaid card                              30.42     30.42\n",
                        "Money transfers                                           0.42      0.42\n",
                        "Payday loan, title loan, or personal loan                 4.82      4.83\n",
                        "Payday loan, title loan, personal loan, or adva...        2.49      2.49\n",
                        "Sample size for indexing: 10,000\n"
                    ]
                }
            ],
            "source": [
                "# Select a stratified sample of 10k records\n",
                "target_sample_size = 10000\n",
                "print(f\"Applying stratified sampling (Target: {target_sample_size})...\")\n",
                "df = create_stratified_sample(df, target_size=target_sample_size)\n",
                "\n",
                "# Verify final shape\n",
                "print(f\"Sample size for indexing: {len(df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "84b8bfe8",
            "metadata": {},
            "source": [
                "## 2. Convert to LangChain Documents\n",
                "\n",
                "We use `src.docs` to convert rows into structured objects that LangChain understands, preserving metadata for retrieval."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "a859c2e4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Converted 10,000 rows to LangChain Documents\n",
                        "  Sample metadata keys: ['complaint_id', 'product', 'sub_product', 'issue', 'sub_issue', 'company', 'state', 'date_received', 'timely_response', 'consumer_disputed']\n",
                        "============================================================\n",
                        "DOCUMENT SAMPLE\n",
                        "============================================================\n",
                        "Content:\n",
                        "i attempted to make reservations for a hotel in xxxx on xx xx xxxx. i received a text notification that it might be fraudulent. i approved the charge and it when through. a subsequent charge was decli...\n",
                        "------------------------------------------------------------\n",
                        "Metadata:\n",
                        "  complaint_id: 5266207\n",
                        "  product: Credit card or prepaid card\n",
                        "  sub_product: General-purpose credit card or charge card\n",
                        "  issue: Trouble using your card\n",
                        "  sub_issue: Can't use card to make purchases\n",
                        "  company: CITIBANK, N.A.\n",
                        "  state: WA\n",
                        "  date_received: 2022-02-27\n",
                        "  timely_response: Yes\n",
                        "  consumer_disputed: None\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "docs = dataframe_to_documents(df)\n",
                "\n",
                "# Preview a document\n",
                "if docs:\n",
                "    print_document_sample(docs[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "edb60d52",
            "metadata": {},
            "source": [
                "## 3. Chunk the Documents\n",
                "\n",
                "Break long narratives into manageable pieces for better embedding search accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "8ab5f67a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[OK] Created text splitter (chunk_size=500, overlap=50)\n",
                        "[OK] Chunking complete:\n",
                        "  Original documents: 10,000\n",
                        "  After chunking: 31,643\n",
                        "  Expansion ratio: 3.16x\n",
                        "\n",
                        "Chunking Stats: {'total_chunks': 31643, 'min_length': 3, 'max_length': 500, 'mean_length': 372.8, 'median_length': 412}\n"
                    ]
                }
            ],
            "source": [
                "# Splitting documents into chunks\n",
                "chunks = chunk_documents(docs)\n",
                "\n",
                "# Display chunk stats\n",
                "stats = get_chunk_stats(chunks)\n",
                "print(f\"\\nChunking Stats: {stats}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "83727f03",
            "metadata": {},
            "source": [
                "## 4. Create Embedding and Vector Store (FAISS)\n",
                "\n",
                "This step converts text into high-dimensional vectors and stores them in a local index."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "9bc4a65d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating vector store with 31,643 documents...\n",
                        "  Persist directory: c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\vector_store\\faiss\n",
                        "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
                        "  (First run will download ~80MB to c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\models\\hf)\n",
                        "✓ Embedding model loaded\n",
                        "✓ FAISS index built (ntotal=31,643)\n",
                        "✓ Vector store persisted to c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\vector_store\\faiss\n",
                        "✓ Vector store built and saved to disk.\n"
                    ]
                }
            ],
            "source": [
                "# Create and persist vector store\n",
                "# Note: On the first run, this download the model (~80MB)\n",
                "vectorstore = create_vector_store(chunks)\n",
                "print(\"✓ Vector store built and saved to disk.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81820f2f",
            "metadata": {},
            "source": [
                "## 5. Test Loading and Retrieval\n",
                "\n",
                "Verify that we can reload the index from disk and perform a search."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "81557b4f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading vector store from c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\vector_store\\faiss...\n",
                        "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
                        "  (First run will download ~80MB to c:\\Users\\Acer\\Documents\\KAIM_PROJECT\\TEST\\rag-complaint-chatbot\\models\\hf)\n",
                        "✓ Embedding model loaded\n",
                        "✓ Vector store loaded (ntotal=31,643)\n",
                        "✓ Created retriever (k=3)\n",
                        "======================================================================\n",
                        "SEARCH RESULTS for: 'unauthorized charge on my credit card'\n",
                        "======================================================================\n",
                        "\n",
                        "--- Result 1 ---\n",
                        "Complaint ID: N/A\n",
                        "Product: N/A\n",
                        "Issue: N/A\n",
                        "Company: N/A\n",
                        "Chunk Index: 4\n",
                        "Content preview:\n",
                        ". furthermore, i did not have the credit card in my physical presence and eyesight the entire time i was in xxxx ( xx xx xxxx-xx xx xxxx ). i handed the card over to merchants on several occasions for them to run it through for other charges that i recognize ( like for a dinner or gas ). so, it is p...\n",
                        "\n",
                        "--- Result 2 ---\n",
                        "Complaint ID: N/A\n",
                        "Product: N/A\n",
                        "Issue: N/A\n",
                        "Company: N/A\n",
                        "Chunk Index: 0\n",
                        "Content preview:\n",
                        "my card was charge unauthorize. but credit card company is not willing to correct this charge....\n",
                        "\n",
                        "--- Result 3 ---\n",
                        "Complaint ID: N/A\n",
                        "Product: N/A\n",
                        "Issue: N/A\n",
                        "Company: N/A\n",
                        "Chunk Index: 0\n",
                        "Content preview:\n",
                        "there has been unauthorized charges of 2600.00 on a credit card that i have known as last four ( xxxx ) xxxx citi simplicity credit card, i have contacted their customer care and their security department indicating them of all these bogus charges and to this date nothing has been done about resolvi...\n",
                        "\n",
                        "======================================================================\n"
                    ]
                }
            ],
            "source": [
                "# Test loading from disk\n",
                "vectorstore_v2 = load_vector_store()\n",
                "\n",
                "# Test retrieval\n",
                "query = \"unauthorized charge on my credit card\"\n",
                "retriever = get_retriever(vectorstore_v2, k=3)\n",
                "results = retriever.invoke(query)\n",
                "\n",
                "# Display results\n",
                "print_search_results(results, query)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d7808e90",
            "metadata": {},
            "source": [
                "## 6. Explore Vector Store\n",
                "\n",
                "A quick look into the index content."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "acec8d7a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total vectors in index: 31,643\n",
                        "Sample chunk metadata from retriever result:\n",
                        "{'complaint_id': 3374042, 'product': 'Credit card or prepaid card', 'sub_product': 'General-purpose credit card or charge card', 'issue': 'Problem with a purchase shown on your statement', 'sub_issue': 'Card was charged for something you did not purchase with the card', 'company': 'CAPITAL ONE FINANCIAL CORPORATION', 'state': 'IL', 'date_received': '2019-09-13', 'timely_response': 'Yes', 'consumer_disputed': None, 'chunk_index': 4}\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Total vectors in index: {vectorstore_v2.index.ntotal:,}\")\n",
                "print(f\"Sample chunk metadata from retriever result:\")\n",
                "print(results[0].metadata)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv (3.11.9)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
