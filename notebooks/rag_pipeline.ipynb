{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "rag-title",
            "metadata": {},
            "source": [
                "# RAG Pipeline: Evaluation and Analysis\n",
                "\n",
                "This notebook focuses on running the complete Retrieval-Augmented Generation (RAG) pipeline on a set of representative questions. We will analyze how the system retrieves relevant complaints and generates answers using the LLM.\n",
                "\n",
                "### Pipeline Components:\n",
                "1. **Vector Store**: FAISS index containing complaint embeddings.\n",
                "2. **Retriever**: Similarity search mechanism to find top-K relevant documents.\n",
                "3. **LLM**: Instruction-tuned model (FLAN-T5) to generate answers based on context.\n",
                "4. **Prompt Engineering**: Structured templates to guide the LLM's response."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag-setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "import IPython\n",
                "from IPython.display import display, Markdown\n",
                "\n",
                "# allow imports from project root\n",
                "project_root = Path.cwd().parent\n",
                "sys.path.insert(0, str(project_root))\n",
                "\n",
                "from src import config, vectorstore, llm\n",
                "from src.rag_pipline import RAGPipeline\n",
                "\n",
                "config.setup_hf_cache()\n",
                "print(\"✓ Environment setup complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rag-init",
            "metadata": {},
            "source": [
                "## 1. Initialize Production Pipeline\n",
                "\n",
                "We initialize the `RAGPipeline` class, which encapsulates the loading of the vector store and the LLM engine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag-init-exec",
            "metadata": {},
            "outputs": [],
            "source": [
                "pipeline = RAGPipeline()\n",
                "if pipeline.initialize():\n",
                "    print(\"✓ RAG Pipeline initialized and ready!\")\n",
                "else:\n",
                "    print(\"❌ Pipeline initialization failed. Check your vector store and model paths.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rag-questions",
            "metadata": {},
            "source": [
                "## 2. Representative Evaluation Questions\n",
                "\n",
                "Here is a curated list of questions that represent common consumer concerns. We will use these to evaluate the system's performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag-questions-list",
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_questions = [\n",
                "    \"What are common issues consumers face with credit card interest rates?\",\n",
                "    \"How do customers describe problems with mortgage loan modifications?\",\n",
                "    \"What are the most frequent complaints regarding unauthorized transactions on checking accounts?\",\n",
                "    \"What reasons do consumers give for disputes regarding credit reporting inaccuracies?\",\n",
                "    \"What are the common themes in complaints against debt collection agencies' practices?\",\n",
                "    \"How do consumers react to unexpected fees on prepaid cards?\",\n",
                "    \"What are the typical complaints about vehicle loan servicing?\",\n",
                "    \"What issues do consumers report when trying to close their savings accounts?\"\n",
                "]\n",
                "\n",
                "print(f\"✓ Defined {len(eval_questions)} representative questions.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rag-execution",
            "metadata": {},
            "source": [
                "## 3. Running RAG Pipeline and Results Analysis\n",
                "\n",
                "For each question, we run the full RAG cycle:\n",
                "1. **Retrieve**: Find top 5 relevant complaint chunks.\n",
                "2. **Augment**: Format the chunks into a context string for the prompt.\n",
                "3. **Generate**: Use FLAN-T5 to generate a professional answer based *only* on the context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag-run-loop",
            "metadata": {},
            "outputs": [],
            "source": [
                "all_results = []\n",
                "\n",
                "for i, query in enumerate(eval_questions, 1):\n",
                "    display(Markdown(f\"### Question {i}: {query}\"))\n",
                "    \n",
                "    # Run the pipeline\n",
                "    result = pipeline.run(query, k=5)\n",
                "    \n",
                "    # Display the answer\n",
                "    display(Markdown(f\"**Generated Answer:** {result['answer']}\"))\n",
                "    \n",
                "    # Display source metadata summary\n",
                "    sources = result['source_documents']\n",
                "    display(Markdown(f\"*Retrieved {len(sources)} sources from products: {set([doc.metadata.get('product', 'N/A') for doc in sources])}*\"))\n",
                "    \n",
                "    # Show a snippet of the most relevant source\n",
                "    top_source = sources[0]\n",
                "    display(Markdown(f\"**Top Source Snippet (Complaint {top_source.metadata.get('complaint_id')}):**  \\n*{top_source.page_content[:250]}...*\"))\n",
                "    \n",
                "    display(Markdown(\"---\"))\n",
                "    \n",
                "    all_results.append({\n",
                "        \"question\": query,\n",
                "        \"answer\": result['answer'],\n",
                "        \"sources\": sources\n",
                "    })"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rag-evaluation-table",
            "metadata": {},
            "source": [
                "## 4. Pipeline Evaluation Table\n",
                "\n",
                "We consolidate the results into a structured evaluation table. This can be exported to your report. \n",
                "\n",
                "> **Note:** The `Quality Score` and `Comments/Analysis` columns are intended for manual review to ensure the AI's answers are grounded and accurate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag-table-exec",
            "metadata": {},
            "outputs": [],
            "source": [
                "evaluation_data = []\n",
                "\n",
                "for res in all_results:\n",
                "    # Get 1-2 source IDs\n",
                "    source_ids = [doc.metadata.get('complaint_id', 'N/A') for doc in res['sources'][:2]]\n",
                "    \n",
                "    evaluation_data.append({\n",
                "        \"Question\": res['question'],\n",
                "        \"Generated Answer\": res['answer'],\n",
                "        \"Retrieved Sources\": \", \".join(map(str, source_ids)),\n",
                "        \"Quality Score (1-5)\": \"\",  # Placeholder for manual review\n",
                "        \"Comments/Analysis\": \"\"      # Placeholder for manual review\n",
                "    })\n",
                "\n",
                "df_eval = pd.DataFrame(evaluation_data)\n",
                "\n",
                "# Set column display for better readability in the notebook\n",
                "pd.set_option('display.max_colwidth', 300)\n",
                "display(df_eval)\n",
                "\n",
                "# Optional: Save to CSV for the report\n",
                "# df_eval.to_csv(\"rag_evaluation_report.csv\", index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}